{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdOQiHPT4K0r"
   },
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UvPMP5wG4K0u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.mode.chained_assignment = None # Fuck warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UsIW6F2to3uX",
    "outputId": "e71e6ec4-deca-44bf-d273-383aba536b08"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn3oSbmH-Vu5"
   },
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9ZDU4M9s4K0y"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(r'snli_1.0_train.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "result = []\n",
    "\n",
    "num = -1 # Сколько данных забрать (-1 чтобы забрать всё)\n",
    "\n",
    "if num < 0:\n",
    "    num = float('inf')\n",
    "\n",
    "num = min(len(json_list), num)\n",
    "\n",
    "for j in range(num):\n",
    "    result.append(json.loads(json_list[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I4H1J3qm4K0z"
   },
   "outputs": [],
   "source": [
    "df_train = pd.json_normalize(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-L8Fwghb4K0z"
   },
   "outputs": [],
   "source": [
    "df_train = df_train[['gold_label', 'sentence1', 'sentence2', 'sentence1_binary_parse', 'sentence2_binary_parse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-75Cr9rd4K00",
    "outputId": "3a852946-c7d4-40a1-995b-12504d418442"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gold_label', 'sentence1', 'sentence2', 'sentence1_binary_parse',\n",
       "       'sentence2_binary_parse'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCVQ6GqG4K02"
   },
   "source": [
    "## Очистка от необозначенных классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RZ-LGPsg4K02"
   },
   "outputs": [],
   "source": [
    "omission_mask = df_train['gold_label']== '-'\n",
    "df_train = df_train.loc[~omission_mask]\n",
    "df_train = df_train.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenght = len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Очистка от бесконечностей в смысловых расстояниях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is training his horse for a competition.</td>\n",
       "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
       "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is at a diner, ordering an omelette.</td>\n",
       "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
       "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entailment</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is outdoors, on a horse.</td>\n",
       "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
       "      <td>( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>They are smiling at their parents</td>\n",
       "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
       "      <td>( They ( are ( smiling ( at ( their parents ) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entailment</td>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>There are children present</td>\n",
       "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
       "      <td>( There ( ( are children ) present ) )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549276</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>Four dirty and barefooted children.</td>\n",
       "      <td>four kids won awards for 'cleanest feet'</td>\n",
       "      <td>( ( ( ( Four dirty ) and ) ( barefooted childr...</td>\n",
       "      <td>( ( four kids ) ( ( won awards ) ( ( ( for ` )...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549277</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Four dirty and barefooted children.</td>\n",
       "      <td>four homeless children had their shoes stolen,...</td>\n",
       "      <td>( ( ( ( Four dirty ) and ) ( barefooted childr...</td>\n",
       "      <td>( ( ( ( ( ( four ( homeless children ) ) ( had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549278</th>\n",
       "      <td>neutral</td>\n",
       "      <td>A man is surfing in a bodysuit in beautiful bl...</td>\n",
       "      <td>A man in a bodysuit is competing in a surfing ...</td>\n",
       "      <td>( ( A man ) ( ( is ( surfing ( in ( ( a bodysu...</td>\n",
       "      <td>( ( ( A man ) ( in ( a bodysuit ) ) ) ( ( is (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549279</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>A man is surfing in a bodysuit in beautiful bl...</td>\n",
       "      <td>A man in a business suit is heading to a board...</td>\n",
       "      <td>( ( A man ) ( ( is ( surfing ( in ( ( a bodysu...</td>\n",
       "      <td>( ( ( A man ) ( in ( a ( business suit ) ) ) )...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549280</th>\n",
       "      <td>entailment</td>\n",
       "      <td>A man is surfing in a bodysuit in beautiful bl...</td>\n",
       "      <td>On the beautiful blue water there is a man in ...</td>\n",
       "      <td>( ( A man ) ( ( is ( surfing ( in ( ( a bodysu...</td>\n",
       "      <td>( ( On ( the ( beautiful ( blue water ) ) ) ) ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549281 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           gold_label                                          sentence1  \\\n",
       "0             neutral  A person on a horse jumps over a broken down a...   \n",
       "1       contradiction  A person on a horse jumps over a broken down a...   \n",
       "2          entailment  A person on a horse jumps over a broken down a...   \n",
       "3             neutral              Children smiling and waving at camera   \n",
       "4          entailment              Children smiling and waving at camera   \n",
       "...               ...                                                ...   \n",
       "549276  contradiction                Four dirty and barefooted children.   \n",
       "549277        neutral                Four dirty and barefooted children.   \n",
       "549278        neutral  A man is surfing in a bodysuit in beautiful bl...   \n",
       "549279  contradiction  A man is surfing in a bodysuit in beautiful bl...   \n",
       "549280     entailment  A man is surfing in a bodysuit in beautiful bl...   \n",
       "\n",
       "                                                sentence2  \\\n",
       "0       A person is training his horse for a competition.   \n",
       "1           A person is at a diner, ordering an omelette.   \n",
       "2                       A person is outdoors, on a horse.   \n",
       "3                       They are smiling at their parents   \n",
       "4                              There are children present   \n",
       "...                                                   ...   \n",
       "549276           four kids won awards for 'cleanest feet'   \n",
       "549277  four homeless children had their shoes stolen,...   \n",
       "549278  A man in a bodysuit is competing in a surfing ...   \n",
       "549279  A man in a business suit is heading to a board...   \n",
       "549280  On the beautiful blue water there is a man in ...   \n",
       "\n",
       "                                   sentence1_binary_parse  \\\n",
       "0       ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
       "1       ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
       "2       ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
       "3       ( Children ( ( ( smiling and ) waving ) ( at c...   \n",
       "4       ( Children ( ( ( smiling and ) waving ) ( at c...   \n",
       "...                                                   ...   \n",
       "549276  ( ( ( ( Four dirty ) and ) ( barefooted childr...   \n",
       "549277  ( ( ( ( Four dirty ) and ) ( barefooted childr...   \n",
       "549278  ( ( A man ) ( ( is ( surfing ( in ( ( a bodysu...   \n",
       "549279  ( ( A man ) ( ( is ( surfing ( in ( ( a bodysu...   \n",
       "549280  ( ( A man ) ( ( is ( surfing ( in ( ( a bodysu...   \n",
       "\n",
       "                                   sentence2_binary_parse  \n",
       "0       ( ( A person ) ( ( is ( ( training ( his horse...  \n",
       "1       ( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...  \n",
       "2       ( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...  \n",
       "3       ( They ( are ( smiling ( at ( their parents ) ...  \n",
       "4                  ( There ( ( are children ) present ) )  \n",
       "...                                                   ...  \n",
       "549276  ( ( four kids ) ( ( won awards ) ( ( ( for ` )...  \n",
       "549277  ( ( ( ( ( ( four ( homeless children ) ) ( had...  \n",
       "549278  ( ( ( A man ) ( in ( a bodysuit ) ) ) ( ( is (...  \n",
       "549279  ( ( ( A man ) ( in ( a ( business suit ) ) ) )...  \n",
       "549280  ( ( On ( the ( beautiful ( blue water ) ) ) ) ...  \n",
       "\n",
       "[549281 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checker = pd.DataFrame()\n",
    "\n",
    "dis = np.loadtxt('D:\\Monty_Python_lmao\\Fumo parsing\\distances.txt')\n",
    "dis = dis[:lenght]\n",
    "checker[0]= dis\n",
    "\n",
    "infinity_mask = checker[0]>999\n",
    "indexs = df_train.loc[infinity_mask].index\n",
    "\n",
    "df_train = df_train.drop(indexs, axis = 0)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.792598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.064395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.796780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.764126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.007054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549276</th>\n",
       "      <td>0.861570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549277</th>\n",
       "      <td>0.694256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549278</th>\n",
       "      <td>0.669675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549279</th>\n",
       "      <td>1.108610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549280</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549281 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0       0.792598\n",
       "1       1.064395\n",
       "2       0.796780\n",
       "3       0.764126\n",
       "4       1.007054\n",
       "...          ...\n",
       "549276  0.861570\n",
       "549277  0.694256\n",
       "549278  0.669675\n",
       "549279  1.108610\n",
       "549280  0.000000\n",
       "\n",
       "[549281 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checker = checker.drop(indexs, axis = 0)\n",
    "checker =checker.reset_index(drop=True)\n",
    "checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bpgxgl6J_lIh"
   },
   "source": [
    "# Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHBhtc1q4K03"
   },
   "source": [
    "## Оставляем только основу от слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vH5IeaQ74K03"
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "\n",
    "\n",
    "my_stem = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    new_sentence = ''\n",
    "\n",
    "    for word in sentence.split():\n",
    "        new_sentence += my_stem.stem(word) + ' '\n",
    "    \n",
    "    return new_sentence[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6aNhSJR4K03"
   },
   "source": [
    "## Убираем символы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nmZRpglO4K04"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def deleteSymbs(sentence):\n",
    "    res = ''\n",
    "\n",
    "    for char in sentence:\n",
    "        if char not in string.punctuation or char in ['(', ')']:\n",
    "            res += char.lower()\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxBwVMXu4K05"
   },
   "source": [
    "## Убираем не несущие смысла слова "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "r7jFt-qp4K05"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def deleteStopWords(sentence):\n",
    "    res = ''\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        if word not in stopwords.words('english'):\n",
    "            res += word + ' '\n",
    "    \n",
    "    return res[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzFvMPLDt0dn"
   },
   "source": [
    "## Считаем нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "j08aSMLmt5xc"
   },
   "outputs": [],
   "source": [
    "def isNeg(sentence):\n",
    "  counter = 0\n",
    "  for word in sentence.split():\n",
    "    if word in ['not', 'no', 'neither', 'none', 'nowhere', 'never', 'nothing']:\n",
    "      counter += 1\n",
    "    elif 'n`t' in word:\n",
    "      counter += 1\n",
    "  return (counter + 1) // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKFks7zi65EC"
   },
   "source": [
    "## Сколько одинаковых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "f87l3f8P7EeW"
   },
   "outputs": [],
   "source": [
    "def countSameWords(sentence1, sentence2):\n",
    "  counter = 0\n",
    "  for word in sentence1.split():\n",
    "    if my_stem.stem(word) in stemSentence(sentence2).split():\n",
    "      counter += 1\n",
    "  return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QzPrEfutr9m"
   },
   "source": [
    "## Подсчёт слов в предложении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9v4PP2c9txxq"
   },
   "outputs": [],
   "source": [
    "def countWords(sentence):\n",
    "  return len(sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKevdMy_Vz9D"
   },
   "source": [
    "## Работаем со скобками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XtF_m5hiV6gk"
   },
   "outputs": [],
   "source": [
    "def getToCloseBracketPart(num_sym, sentence):\n",
    "    while num_sym < len(sentence) and sentence[num_sym] != '(':\n",
    "        num_sym += 1\n",
    "\n",
    "    if num_sym == len(sentence):\n",
    "        return 'Not a bracket, you fucker'\n",
    "\n",
    "    sentence = sentence[num_sym + 1:]\n",
    "    bracket_counter = 1\n",
    "    for i, char in enumerate(sentence):\n",
    "        if char == '(':\n",
    "            bracket_counter += 1\n",
    "        elif char == ')':\n",
    "            bracket_counter -= 1\n",
    "            if bracket_counter == 0:\n",
    "                sentence = sentence[:i]\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def delSpaces(sentence):\n",
    "    res = ''\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        res += sentence[i]\n",
    "        if sentence[i] == '(':\n",
    "            i += 1\n",
    "        elif sentence[i] == ')':\n",
    "            res = res[:-2] + ')'\n",
    "        i += 1\n",
    "    return res\n",
    "\n",
    "\n",
    "def unpackSentence(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    this_part = ''\n",
    "    parts = []\n",
    "\n",
    "    if sentence[0] != '(':\n",
    "        if '(' not in sentence:\n",
    "            return [sentence]\n",
    "        else:\n",
    "            sentence = '(' + sentence[:sentence.find('(')] + ')' + sentence[sentence.find('('):]\n",
    "\n",
    "    bracket_counter = 0\n",
    "    for i, char in enumerate(sentence):\n",
    "        if char == \"(\":\n",
    "            bracket_counter += 1\n",
    "            this_part += char\n",
    "        elif char == \")\":\n",
    "            this_part += char\n",
    "            bracket_counter -= 1\n",
    "            if bracket_counter == 0:\n",
    "                parts.append(this_part.strip())\n",
    "                this_part = ''\n",
    "        else:\n",
    "            this_part += char\n",
    "\n",
    "    if this_part != '':\n",
    "      parts.append(f'({this_part.strip()})')\n",
    "    new_parts = []\n",
    "    for part in parts:\n",
    "        if part != '':\n",
    "            new_parts.append(part[1:-1])\n",
    "    return new_parts\n",
    "\n",
    "\n",
    "def indexMaxWords(array):\n",
    "    index = 0\n",
    "    maxis = 0\n",
    "    for i, el in enumerate(array):\n",
    "        if len(el.split()) > maxis:\n",
    "            index = i\n",
    "            maxis = len(el.split())\n",
    "    return index\n",
    "\n",
    "\n",
    "def unifySentences(sentence1, sentence2):\n",
    "    smaller = unpackSentence(sentence1)\n",
    "    bigger = unpackSentence(sentence2)\n",
    "\n",
    "    if len(bigger) < len(smaller):\n",
    "        bigger, smaller = smaller, bigger\n",
    "\n",
    "    while len(smaller) != len(bigger) or len(smaller) < 3:\n",
    "        index = indexMaxWords(smaller)\n",
    "        new_parts = unpackSentence(smaller[index])\n",
    "        if new_parts[0] == smaller[index]:\n",
    "          new_parts.append(smaller[index])\n",
    "        del smaller[index]\n",
    "        smaller.extend(new_parts)\n",
    "\n",
    "        if len(bigger) < len(smaller):\n",
    "            bigger, smaller = smaller, bigger\n",
    "    return bigger, smaller\n",
    "\n",
    "\n",
    "def AllPossiblitieses():\n",
    "    res = []\n",
    "    for i in range(3):\n",
    "        vec = [i, -1, -1]\n",
    "        for j in set(range(3)) - set([i]):\n",
    "            c = list(set(range(3)) - set([i]) - set([j]))[0]\n",
    "            vec[1] = j\n",
    "            vec[2] = c\n",
    "            res.append(tuple(vec))\n",
    "    return res\n",
    "\n",
    "\n",
    "def getMeaningVecs(sentence1, sentence2):\n",
    "  sentence1, sentence2 = unifySentences(sentence1, sentence2)\n",
    "  sentence1 = list(map(getSentenceVecSum, sentence1))\n",
    "  sentence2 = list(map(getSentenceVecSum, sentence2))\n",
    "\n",
    "  sums = []\n",
    "  possibilities = AllPossiblitieses()\n",
    "  for pos in possibilities:\n",
    "    sum = 0\n",
    "    for i in range(3):\n",
    "      take = sentence1[i] - sentence2[pos[i]]\n",
    "      take = np.array(take)\n",
    "      sum += np.sum(sum)\n",
    "    sums.append(sum)\n",
    "  \n",
    "  sums = np.array(sums)\n",
    "  best_pos = possibilities[np.argmin(sums)]\n",
    "\n",
    "  vecs = []\n",
    "  for i in range(3):\n",
    "    vecs.append(sentence1[i] - sentence2[best_pos[i]])\n",
    "  \n",
    "  return vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9msKHQtV4K05"
   },
   "source": [
    "## Agregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6EeOu3kk4K06"
   },
   "outputs": [],
   "source": [
    "def fixSentence(sentence, funcs):   # funcs должно включать вышеописанные функции\n",
    "    for func in funcs:\n",
    "        sentence = eval(func)(sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xu-HQ7Iu_EZm"
   },
   "source": [
    "# Word2Vek и Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "THPEH2aL4K06"
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "# corpus = api.load('text8')  # download the corpus and return it opened as an iterable\n",
    "# model = Word2Vec(corpus)    # train a model from the corpus\n",
    "# model.most_similar(\"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1-0F-Qq4K07",
    "outputId": "995a4c31-ceac-448b-c5ab-a014f3f4035b"
   },
   "outputs": [],
   "source": [
    "# model = api.load('glove-twitter-200') # 758MB\n",
    "model = api.load('glove-wiki-gigaword-100') # 128MB - гораздо адекватнее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "s8ocxpX2pmfX"
   },
   "outputs": [],
   "source": [
    "for i in [1, 2]:\n",
    "    df_train[f'Fixed_sentence{i}'] = df_train[f'sentence{i}'].apply(lambda sentence: fixSentence(sentence, ['deleteSymbs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZCry9VGM6S_P"
   },
   "outputs": [],
   "source": [
    "for i in [1, 2]:\n",
    "    df_train[f'Fixed_bin_sentence{i}'] = df_train[f'sentence{i}_binary_parse'].apply(lambda sentence: fixSentence(sentence, ['deleteSymbs', 'delSpaces']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UAG3HOxY4K07",
    "outputId": "0ef720fc-2897-42a8-a939-f12aba26c787",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horses', 0.8550404906272888),\n",
       " ('riding', 0.7131650447845459),\n",
       " ('dog', 0.7109653353691101),\n",
       " ('racing', 0.6629325151443481),\n",
       " ('bike', 0.6614626049995422),\n",
       " ('thoroughbred', 0.6600428819656372),\n",
       " ('bull', 0.6456228494644165),\n",
       " ('jockey', 0.6354342699050903),\n",
       " ('breeders', 0.6333900690078735),\n",
       " ('race', 0.6270371675491333)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('horse') # Проверка работы скачанной модели word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGeqQQib4K08"
   },
   "source": [
    "## Подготовка данных для w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "eg-PeFh9wWL0"
   },
   "outputs": [],
   "source": [
    "whole_sentence = df_train['Fixed_sentence1'] + ' : ' + df_train['Fixed_sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "JEroAbjx--77"
   },
   "outputs": [],
   "source": [
    "whole_bin_sentence = df_train['Fixed_bin_sentence1'] + ' : ' + df_train['Fixed_bin_sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "GoG3qAyWwkWD"
   },
   "outputs": [],
   "source": [
    "neg_dif = whole_sentence.apply(isNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWvIRDpmCH45"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "TNhFZmsu7k3b"
   },
   "outputs": [],
   "source": [
    "words_counter = whole_sentence.apply(lambda sentence: countWords(sentence.split(' : ')[0]) / countWords(sentence.split(' : ')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6Xp9Rlyj4K09"
   },
   "outputs": [],
   "source": [
    "def getSentenceVecSum(sentence):\n",
    "  global model\n",
    "  sentence = sentence.replace(')', ' ')\n",
    "  sentence = sentence.replace('(', ' ')\n",
    "\n",
    "  result_vec = np.zeros(100)\n",
    "  for word in sentence.split():\n",
    "      try:\n",
    "          result_vec += model.wv[word]\n",
    "      except:\n",
    "          pass\n",
    "    \n",
    "  return result_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1x20JhSTAlGi",
    "outputId": "2759bf36-1d93-4b00-c955-b9ca591dbf93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-010c5506599d>:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  result_vec += model.wv[word]\n"
     ]
    }
   ],
   "source": [
    "new_meaning_vecs = whole_bin_sentence.apply(lambda sentence: getMeaningVecs(sentence.split(' : ')[0], sentence.split(' : ')[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zx1DqBNv4K08"
   },
   "source": [
    "## Word2Vec + Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "tLF4nzST4K09"
   },
   "outputs": [],
   "source": [
    "def getDistanceBetweenVecs(vec1, vec2):\n",
    "    return np.linalg.norm(vec1-vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNovnp6D4K09"
   },
   "source": [
    "## Логистическая регрессия по смыслу слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSaWE1fUI1I1",
    "outputId": "3eafa309-ac02-4491-8007-8fb1c9af9ad7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-010c5506599d>:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  result_vec += model.wv[word]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "sentence1 = df_train.Fixed_sentence1.apply(lambda sentence: getSentenceVecSum(sentence))\n",
    "sentence2 = df_train.Fixed_sentence2.apply(lambda sentence: getSentenceVecSum(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "nLpJL2GIOIMQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [0.2302660197019577, 0.660622013732791, 1.4518...\n",
       "1         [1.0350759774446487, 2.03613199852407, 0.72746...\n",
       "2         [1.4307060092687607, 0.8914919998496771, 0.777...\n",
       "3         [0.517536997795105, -0.06919004023075104, -0.7...\n",
       "4         [1.4938320070505142, -1.456748977303505, -0.36...\n",
       "                                ...                        \n",
       "549276    [-1.1741630211472511, -0.9560000151395798, -2....\n",
       "549277    [-2.554003030061722, -2.694529950618744, -2.01...\n",
       "549278    [-1.287889003753662, 1.396524053066969, 0.6614...\n",
       "549279    [0.04770695045590401, 3.1734850481152534, -0.4...\n",
       "549280    [0.4267420023679733, -0.24646998941898346, -1....\n",
       "Length: 549281, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sentence1 - sentence2\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка даннных для глобальной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "TIz6boKK7u1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Данные подготовлены для линейной регрессии\n"
     ]
    }
   ],
   "source": [
    "new_x = []\n",
    "for i in range(len(x)):\n",
    "    new_x.append([])\n",
    "    for j in range(len(x[0])):\n",
    "        new_x[i].append(x[i][j])\n",
    "\n",
    "X = pd.DataFrame(new_x)\n",
    "\n",
    "y = df_train.gold_label.apply(lambda label: {'entailment': 1, 'contradiction': -1, 'neutral': 0, '-': 3}[label])\n",
    "print('>> Данные подготовлены для линейной регрессии')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "2tDwjiqu-Yo3"
   },
   "outputs": [],
   "source": [
    "series1 = new_meaning_vecs.apply(lambda x: x[0])\n",
    "series2 = new_meaning_vecs.apply(lambda x: x[1])\n",
    "series3 = new_meaning_vecs.apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "k7rNvq--K7c8"
   },
   "outputs": [],
   "source": [
    "new_1 = []\n",
    "for i in range(len(series1)):\n",
    "    new_1.append([])\n",
    "    for j in range(len(series1[0])):\n",
    "        new_1[i].append(series1[i][j])\n",
    "\n",
    "new_1 = pd.DataFrame(new_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "E88lZEnN_0ZD"
   },
   "outputs": [],
   "source": [
    "new_2 = []\n",
    "for i in range(len(series2)):\n",
    "    new_2.append([])\n",
    "    for j in range(len(series2[0])):\n",
    "        new_2[i].append(series2[i][j])\n",
    "\n",
    "new_2 = pd.DataFrame(new_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "AM5D6j_m_3Qi"
   },
   "outputs": [],
   "source": [
    "new_3 = []\n",
    "for i in range(len(series3)):\n",
    "    new_3.append([])\n",
    "    for j in range(len(series3[0])):\n",
    "        new_3[i].append(series3[i][j])\n",
    "\n",
    "new_3 = pd.DataFrame(new_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "JT3CNg1svPD7"
   },
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "\n",
    "for j in range(100):\n",
    "  new_dict[j] = 100+j\n",
    "new_2 = new_2.rename(columns=new_dict)\n",
    "\n",
    "for j in range(100):\n",
    "  new_dict[j] = 200+j\n",
    "new_3 = new_3.rename(columns=new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "ON_EqktuAY0M"
   },
   "outputs": [],
   "source": [
    "x = pd.concat([new_1, new_2, new_3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "VJW2XR_sMCyO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Данные подготовлены для psucho vecs регрессии\n"
     ]
    }
   ],
   "source": [
    "print('>> Данные подготовлены для psucho vecs регрессии')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[100] = checker[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "k5xR88SvPhBO"
   },
   "outputs": [],
   "source": [
    "random_seed = 12\n",
    "\n",
    "x_train, x_educ, y_train, y_educ = train_test_split(x, y, test_size=0.3, random_state=random_seed)\n",
    "X_train, X_educ, y_train, y_educ = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение логистической регрессии на psucho vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ZPNO0lXlw2_3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression()\n",
    "preds1 = clf1.fit(x_educ, y_educ).predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7BBxWUkwsoy"
   },
   "source": [
    "## Обучение Гуасса для смыслового расстояния"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "vUuDVMavMYbW"
   },
   "outputs": [],
   "source": [
    "clf2 =  SVC(gamma='auto')\n",
    "preds2 = clf2.fit(X_educ.head(5000), y_educ.head(5000)).predict(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение финальной логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[101] = preds1\n",
    "X_train[102] = preds2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      ">> Результаты кроссвалидации: [0.55137841 0.55223085 0.55219184 0.55125554 0.55484467]\n",
      "________________________________________________________________________________\n",
      "0.5523802614379549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logRegr = LogisticRegression()\n",
    "cv_results = cross_val_score(logRegr, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print('_'*80, f'>> Результаты кроссвалидации: {cv_results}', '_'*80, sep='\\n')\n",
    "print(cv_results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение финальной модели и прогнозирование на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'snli_1.0_test.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "result = []\n",
    "\n",
    "num = -1 # Сколько данных забрать (-1 чтобы забрать всё)\n",
    "\n",
    "if num < 0:\n",
    "    num = float('inf')\n",
    "\n",
    "num = min(len(json_list), num)\n",
    "\n",
    "for j in range(num):\n",
    "    result.append(json.loads(json_list[j]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.json_normalize(result)\n",
    "df_test = df_test[['gold_label', 'sentence1', 'sentence2', 'sentence1_binary_parse', 'sentence2_binary_parse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker2 = pd.DataFrame()\n",
    "\n",
    "dis2 = np.loadtxt('D:\\Monty_Python_lmao\\Fumo parsing\\distances_test.txt')\n",
    "checker2[0]= dis2\n",
    "checker2[checker2[0]>999] = 1.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1, 2]:\n",
    "    df_test[f'Fixed_sentence{i}'] = df_test[f'sentence{i}'].apply(lambda sentence: fixSentence(sentence, ['deleteSymbs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1, 2]:\n",
    "    df_test[f'Fixed_bin_sentence{i}'] = df_test[f'sentence{i}_binary_parse'].apply(lambda sentence: fixSentence(sentence, ['deleteSymbs', 'delSpaces']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_sentence_test = df_test['Fixed_sentence1'] + ' : ' + df_test['Fixed_sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_bin_sentence_test = df_test['Fixed_bin_sentence1'] + ' : ' + df_test['Fixed_bin_sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dif_test = whole_sentence_test.apply(isNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_counter_test = whole_sentence_test.apply(lambda sentence: countWords(sentence.split(' : ')[0]) / countWords(sentence.split(' : ')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-010c5506599d>:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  result_vec += model.wv[word]\n"
     ]
    }
   ],
   "source": [
    "new_meaning_vecs_test = whole_bin_sentence_test.apply(lambda sentence: getMeaningVecs(sentence.split(' : ')[0], sentence.split(' : ')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-010c5506599d>:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  result_vec += model.wv[word]\n"
     ]
    }
   ],
   "source": [
    "sentence1_test = df_test.Fixed_sentence1.apply(lambda sentence: getSentenceVecSum(sentence))\n",
    "sentence2_test = df_test.Fixed_sentence2.apply(lambda sentence: getSentenceVecSum(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [-1.7302690111100674, 5.868997022509575, 0.578...\n",
       "1       [0.9498959593474865, 5.716138012707233, -0.464...\n",
       "2       [-1.8306270353496075, 5.182091008871794, 0.808...\n",
       "3       [-1.5425489461049438, 2.433895979076624, -2.07...\n",
       "4       [-0.33247298654168844, 2.4271559603512287, -2....\n",
       "                              ...                        \n",
       "9995    [-1.201142966747284, 0.16192601714283228, -1.1...\n",
       "9996    [-1.0620899721980095, 0.03271903097629547, -1....\n",
       "9997    [0.1350870169699192, 2.1642130091786385, 1.134...\n",
       "9998    [-0.7435459904372692, 1.8506419956684113, 0.67...\n",
       "9999    [-1.7904659770429134, 0.6284220367670059, 1.76...\n",
       "Length: 10000, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = sentence1_test - sentence2_test\n",
    "\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Данные подготовлены для линейной регрессии\n"
     ]
    }
   ],
   "source": [
    "new_x = []\n",
    "for i in range(len(x_test)):\n",
    "    new_x.append([])\n",
    "    for j in range(len(x_test[0])):\n",
    "        new_x[i].append(x_test[i][j])\n",
    "\n",
    "X_test = pd.DataFrame(new_x)\n",
    "\n",
    "y_test = df_test.gold_label.apply(lambda label: {'entailment': 1, 'contradiction': -1, 'neutral': 0, '-': 3}[label])\n",
    "print('>> Данные подготовлены для линейной регрессии')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "series1_test = new_meaning_vecs_test.apply(lambda x: x[0])\n",
    "series2_test = new_meaning_vecs_test.apply(lambda x: x[1])\n",
    "series3_test = new_meaning_vecs_test.apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_1_test = []\n",
    "for i in range(len(series1_test)):\n",
    "    new_1_test.append([])\n",
    "    for j in range(len(series1_test[0])):\n",
    "        new_1_test[i].append(series1_test[i][j])\n",
    "\n",
    "new_1_test = pd.DataFrame(new_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_2_test = []\n",
    "for i in range(len(series2_test)):\n",
    "    new_2_test.append([])\n",
    "    for j in range(len(series2_test[0])):\n",
    "        new_2_test[i].append(series2_test[i][j])\n",
    "\n",
    "new_2_test = pd.DataFrame(new_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_3_test = []\n",
    "for i in range(len(series3_test)):\n",
    "    new_3_test.append([])\n",
    "    for j in range(len(series3_test[0])):\n",
    "        new_3_test[i].append(series3_test[i][j])\n",
    "\n",
    "new_3_test = pd.DataFrame(new_3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "\n",
    "for j in range(100):\n",
    "  new_dict[j] = 100+j\n",
    "new_2_test = new_2_test.rename(columns=new_dict)\n",
    "\n",
    "for j in range(100):\n",
    "  new_dict[j] = 200+j\n",
    "new_3_test = new_3_test.rename(columns=new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.concat([new_1_test, new_2_test, new_3_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Данные подготовлены для psucho vecs регрессии\n"
     ]
    }
   ],
   "source": [
    "print('>> Данные подготовлены для psucho vecs регрессии')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[100] = checker2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "preds1_test = clf1.fit(x_train, y_train).predict(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "preds2_test = clf2.fit(X_educ.head(15000), y_educ.head(15000)).predict(X_test)\n",
    "# preds2_test = clf2.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[101] = preds1_test\n",
    "X_test[102] = preds2_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "main_classifier = LogisticRegression()\n",
    "main_classifier = main_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = main_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5624"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, test_pred, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1687,  730,  820,    0],\n",
       "       [ 823, 1606,  790,    0],\n",
       "       [ 547,  490, 2331,    0],\n",
       "       [  51,   51,   74,    0]], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = confusion_matrix(y_test, test_pred)\n",
    "con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5724755700325733"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, test_pred, normalize=False)/ (np.sum(con)- np.sum(con[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wCVQ6GqG4K02",
    "SHBhtc1q4K03",
    "D6aNhSJR4K03",
    "dxBwVMXu4K05",
    "NzFvMPLDt0dn",
    "9msKHQtV4K05"
   ],
   "name": "new_wave.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
